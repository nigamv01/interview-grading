{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a27f91-cefd-4000-86d7-96b37318f888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e8d364-e5a2-4f94-87b1-839a2e3ccb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MLPrompt1(dspy.Signature):\n",
    "    \"\"\" Check if in the provided context a train-test split was done using SparkML APIs \"\"\"\n",
    "\n",
    "    text: str = dspy.InputField()\n",
    "\n",
    "    score: str = dspy.OutputField(desc=\"5 if any code snippet below returns a train-test split using SparkML APIs, 0 if no code snippet returns a train-test split using SparkML APIs\")\n",
    "\n",
    "    code_snippet: str = dspy.OutputField(desc=\"provide the code_snippet which returns a train-test split using SparkML APIs encapsulated as a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "038efdc0-7cf9-4e6c-8bea-a69f8df91649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MLPrompt2(dspy.Signature):\n",
    "  \"\"\" Check if in the provided context if a model trained using SparkML APIs to predict price given other input features or a subset of them \"\"\"\n",
    "\n",
    "  text: str = dspy.InputField()\n",
    "\n",
    "  score: str = dspy.OutputField(desc=\"\"\" \n",
    "                                      30 if a model is trained using SparkML APIs to predict price given other input features or a subset of them,\n",
    "                                      20 if a model is trained using sklearn or any other libraries to predict price given other input features or a subset of them,\n",
    "                                      0 if a model training step does not occur \n",
    "                                \"\"\")\n",
    "\n",
    "  code_snippet: str = dspy.OutputField(desc=\"provide the code_snippet which trains a model using SparkMl APIs or any other library\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6b48ec-cfec-49f5-9bf8-3739052756dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MLPrompt3(dspy.Signature):\n",
    "  \"\"\" Check if multiple models were built with parameter tuning, model evaluation and comparison between them. See if an alortihmic comparison was done \"\"\"\n",
    "\n",
    "  text: str = dspy.InputField()\n",
    "  \n",
    "  score: str = dspy.OutputField(desc=\"\"\" \n",
    "                                      30 if multiple models were built with parameter tuning, model evaluation and comparison between them,\n",
    "                                      25 if multiple models were built and compared but parameter tuning was not done, \n",
    "                                      20 if parameter tuning was done but multiple models were not built and algorithmic comparison was not done,\n",
    "                                      15 if a single model was built with default parameters,\n",
    "                                      0 if a model training step does not occur \n",
    "                                \"\"\")\n",
    "\n",
    "  code_snippet: str = dspy.OutputField(desc=\"provide the code_snippet which performs parameter tuning, model evaluation and comparison between them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "194b372c-1fcf-4908-ae0f-520b133e2eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MLPrompt4(dspy.Signature):\n",
    "  \"\"\" Check if a loss metric was computed on the test dataset and choice of loss metric was explained \"\"\"\n",
    "\n",
    "  text: str = dspy.InputField()\n",
    "  \n",
    "  score: str = dspy.OutputField(desc=\"\"\" \n",
    "                                      5 if loss metric was computed on the test dataset and choice of loss metric was explained,\n",
    "                                      2.5 loss metric was computed on the test dataset but choice of loss metric was not explained, \n",
    "                                      0 if loss metric was not mentioned/computed or if loss metric was computed on training dataset itself\n",
    "                                \"\"\")\n",
    "\n",
    "  code_snippet: str = dspy.OutputField(desc=\"provide the code_snippet which shows loss metric was computed. Also provide the explanation for choosing the loss metric if provided \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9fb6ba3-1114-4f23-af03-69716f347565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MLPrompt5(dspy.Signature):\n",
    "  \"\"\" Check if a model, hyperparemeter and metrics were logged to MLFlow \"\"\"\n",
    "\n",
    "  text: str = dspy.InputField()\n",
    "  \n",
    "  score: str = dspy.OutputField(desc=\"\"\" \n",
    "                                      10 model, hyperparemeter and metrics were logged to MLFlow,\n",
    "                                      deduct 2.5 points if any of the model, hyperparameter or metrics were not logged to MLFlow, \n",
    "                                      0 if mlfloww as not used at all\n",
    "                                \"\"\")\n",
    "\n",
    "  code_snippet: str = dspy.OutputField(desc=\"provide the code_snippet which model, hyperparemeter and metrics were logged to MLFlow \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a131ef-9d83-4324-a7e5-ca9fc2f06f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module_dict_ml = {\n",
    "  'module_1': dspy.ChainOfThought(MLPrompt1),\n",
    "  'module_2': dspy.ChainOfThought(MLPrompt2),\n",
    "  'module_3': dspy.ChainOfThought(MLPrompt3),\n",
    "  'module_4': dspy.ChainOfThought(MLPrompt4),\n",
    "  'module_5': dspy.ChainOfThought(MLPrompt5)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_prompts_with_DSPy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
