{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a82f190-dbec-428c-b8a8-991f921373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nbformat databricks-sdk[openai]==0.38.0 dspy --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555bb1a0-41dd-4982-82f8-08828d0c0dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../prompts/sql_prompts_with_DSPy_playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723a26f7-bf12-4e7d-9626-a95a21c8a5a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../prompts/ml_prompts_with_DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756027f5-9b2a-46ab-820c-7ecc3e9f8cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../prompts/optimization_prompts_with_DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da7e05a-f548-45ec-b1ae-190733a0294b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../helper/GradingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2586d01-753c-40d3-bbd0-feb1e0f24f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module = Module(databricks.sdk.WorkspaceClient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee040399-add3-44b2-ae01-f0f9f5bae0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "candidate_dict = {\n",
    "  \"X\": {\n",
    "    # \"SQL_Q1\": \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-sql/01-SQL-X-no-answer-q1\",\n",
    "    \"SQL_Q2\": \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-sql/sql_notebook_abhay\",\n",
    "    # \"Optimization\": \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-optimization/Optimization-X\",\n",
    "    # \"ML\": \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-ml/04-Machine-Learning-X\"\n",
    "  }\n",
    "}\n",
    "\n",
    "human_graded_dict = {\n",
    "  \"X\": {\n",
    "    # \"SQL_Q1\": [0,15,15,15,15,25],\n",
    "    \"SQL_Q2\": [15,0,15,15,15,25],\n",
    "    # \"Optimization\": [1,0,1,1,1,1,1,0],\n",
    "    # \"ML\": [5, 30, 25, 5, 0]\n",
    "  }\n",
    "}\n",
    "\n",
    "module_list = {\n",
    "  \"SQL\": module_dict_sql,\n",
    "  # \"Optimization\": module_dict_optimization,\n",
    "  # \"ML\": module_dict_ml\n",
    "}\n",
    "\n",
    "llm_models = {\n",
    "  # 'llm-405B': 'databricks/databricks-meta-llama-3-1-405b-instruct',\n",
    "  'llm-70B': 'databricks/databricks-meta-llama-3-3-70b-instruct'\n",
    "  # 'claude': \"databricks/databricks-claude-3-7-sonnet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873dd63e-bb0c-4ca3-b14d-bb7b52bb2997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_scores(predictions, human_grading):\n",
    "  mismatch_dict = {}\n",
    "\n",
    "  scores_list = predictions['score'].tolist()\n",
    "  code_snippet_list = predictions['code_snippet'].tolist()\n",
    "  reasoning_list = predictions['reasoning'].tolist()\n",
    "\n",
    "  for i in range(0, len(human_grading)):\n",
    "    if scores_list[i]!= human_grading[i]:\n",
    "      mismatch_dict[i+1] = {\n",
    "        'predicted_score': scores_list[i],\n",
    "        'human_score': human_grading[i],\n",
    "        'code_snippet': code_snippet_list[i],\n",
    "        'reasoning': reasoning_list[i]\n",
    "      }\n",
    "  if not mismatch_dict:\n",
    "    return None\n",
    "  \n",
    "  return mismatch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d960767a-63a4-4af1-a45a-931837616103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "test_results = {}\n",
    "\n",
    "for i in range(0,5):\n",
    "  model_dict = {}\n",
    "  for k,v in llm_models.items():\n",
    "    start_time = time.time()\n",
    "    for candidate, values in candidate_dict.items():\n",
    "      answers_dict = {}\n",
    "      for section, context_path in values.items():\n",
    "        candidate_mismatch = {}\n",
    "        lm = dspy.LM(llm_models[k], cache=False, temperature=0)\n",
    "        dspy.settings.configure(lm=lm)\n",
    "        module.set_module_dict(module_list[\"SQL\"])\n",
    "        results = module.get_error_and_answer_dict(context_path)\n",
    "        results['answers_dict']['candidate'] = candidate\n",
    "        results['answers_dict']['section'] = section\n",
    "        results['answers_dict']['context'] = results['context'] \n",
    "        answers_dict[section] = results['answers_dict']\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\" time taken by model {k} for execution of section {section} is {execution_time} seconds\")\n",
    "        answers_df = pd.concat(answers_dict.values(), ignore_index=True)\n",
    "        candidate_mismatch[f'{candidate}_{section}'] = evaluate_scores(answers_df, human_graded_dict[candidate][section])\n",
    "\n",
    "    model_dict[k] = {'answers_dict': answers_dict, 'execution_time': execution_time, 'lm':lm, 'candidate_mismatch': candidate_mismatch}\n",
    "    print(f\" run completed {i+1} \\n\")\n",
    "  test_results[f'{k}_round_{i+1}'] = {\n",
    "    # 'model_details': model_dict,\n",
    "    'mismatch': candidate_mismatch\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4edfb8-9d20-49cb-bb9a-86cf56d6a876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fb3b27-d489-47e0-8b90-9b54313b517c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module_list['SQL']['module_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c0ff89-3e85-4d1e-a886-fcedac1dfff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Define signature to be used for metric evaluation \n",
    "# class Answer(dspy.Signature):\n",
    "#   score: int = dspy.OutputField(desc=\"The score provided by the model\")\n",
    "#   instruction: str = dspy.OutputField(desc=\"The instruction given to the llm model\")\n",
    "#   code_snippet: str = dspy.OutputField(desc=\"\"\" supporting code snippet provided by the model for the score \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5a56db-0983-4f39-a062-972452c94d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Read the true answers from a database \n",
    "# true_answers_df = spark.read.table(\"users.abhay_jalisatgi.training_set\").select(\"score\", \"code_snippet\", \"question\").collect()\n",
    "# true_answer_list = [Answer(score=row.score, instruction=row.question, code_snippet=row.code_snippet) for row in true_answers_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ca3514-7c20-460c-a222-b2dcdc5e4405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Evaluate RMSE \n",
    "# def evaluate_score(example, pred, trace=None):\n",
    "#   score_list = {x.instruction: x.score for x in example}\n",
    "#   score_diff_list = [(x['score'] - score_list[x['question']])**2 for x in pred if x['question'] in score_list]\n",
    "#   return np.sqrt(np.mean(score_diff_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17f1f20-a119-4edc-8268-8e824c438aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_score(true_answer_list, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a511ebf1-69c8-42c9-9fc2-b20505183629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Perform exact answer match between the code_snippets \n",
    "# def evaluate_code_snippet(example, pred, trace=None):\n",
    "#   score_list = {x.instruction: x.code_snippet for x in example}\n",
    "#   score_diff_list = {x['question']: (x['code_snippet'] == score_list[x['question']]) for x in pred if x['question'] in score_list}\n",
    "#   return score_diff_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8ced50-d843-4c37-8cf6-589d255452a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# evaluate_code_snippet(true_answer_list, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7782bd8-223b-4fa8-939d-e7b081e88bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "grade_candidate_genai_dspy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
