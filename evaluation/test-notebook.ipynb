{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a82f190-dbec-428c-b8a8-991f921373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nbformat databricks-sdk[openai]==0.38.0 dspy --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d914938a-11cb-41de-a6cb-c523bd28227b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"users.abhay_jalisatgi.few_shots_db\")\n",
    "df_selected = df.select('question', 'context', 'score', 'code_snippet')\n",
    "df_selected.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b0d75d8-1177-4c26-84e2-16c5b0af2ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy \n",
    "class Example(dspy.Signature):\n",
    "  text = dspy.InputField(desc=\"The text to analyze\")\n",
    "  score: int = dspy.OutputField(desc=\"The score provided by the model\")\n",
    "  code_snippet: str = dspy.OutputField(desc=\"\"\" supporting code snippet provided by the model for the score \"\"\")\n",
    "  explanation: str = dspy.OutputField(desc=\"\"\" explanation provided by the model for the score \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "944a5e32-44d8-4ce1-bcbd-ad984aab1535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lm = dspy.LM('databricks/databricks-meta-llama-3-3-70b-instruct')\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4ba1cb-92ea-4a3b-8d3a-7458c17d73fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple signature (you'll likely have a more complex one)\n",
    "class BasicQA(dspy.Signature):\n",
    "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()\n",
    "\n",
    "# Create a module that uses the signature\n",
    "class GenerateAnswer(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(BasicQA)\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.generate_answer(question=question)\n",
    "\n",
    "# Create few-shot examples using dspy.Example\n",
    "train_examples = [\n",
    "    dspy.Example(question=\"What is the capital of France?\", answer=\"Paris\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"What is the highest mountain in the world?\", answer=\"Mount Everest\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"Who painted the Mona Lisa?\", answer=\"Leonardo da Vinci\").with_inputs(\"question\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b74a9a5-bba2-408a-987b-d7af029d6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the module\n",
    "qa_module = GenerateAnswer()\n",
    "\n",
    "# Demonstrate the few-shot examples to the module\n",
    "qa_module.generate_answer.demos = train_examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22dd75b-09f5-4dff-ac71-455b255bb92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SQLPrompt(dspy.Signature):\n",
    "  text = dspy.InputField(desc=\"The text to analyze\")\n",
    "  score: int = dspy.OutputField(desc=\"The score provided by the model\")\n",
    "  code_snippet: str = dspy.OutputField(desc=\"\"\" supporting code snippet provided by the model for the score \"\"\")\n",
    "  explanation: str = dspy.OutputField(desc=\"\"\" explanation provided by the model for the score \"\"\")\n",
    "\n",
    "class GenerateAnswerSQLPrompt(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.ChainOfThought(SQLPrompt)\n",
    "\n",
    "    def forward(self, context):\n",
    "        return self.generate_answer(text=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fffdb5d-2f90-4e9e-a3c2-8ae5ad1be08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training = [dspy.Example(text=row.context, score=row.score, code_snippet = row.code_snippet).with_inputs(\"text\") for row in df_selected.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e361acb-528e-4bf6-a63d-9b84682a32d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the module\n",
    "qa_module = GenerateAnswerSQLPrompt()\n",
    "\n",
    "# Demonstrate the few-shot examples to the module\n",
    "qa_module.generate_answer.demos = training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22bb8d75-7855-49b8-ad48-fee94bdc43d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "context = \"\"\" \n",
    "                              # Databricks Coding Challenge - SQL\n",
    "### Note: All questions should be done using SQL language\n",
    "\n",
    "## Spark SQL and DataFrames \n",
    "\n",
    "In this section, you'll read in data to create a DataFrame in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this [link](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfs) for more details on how to use DBFS.##Understanding the data set \n",
    "\n",
    "###Overview:\n",
    "The data set used throughout the coding assessment resembles telemetry data that any software as a service (SaaS) company might collect. One record represents the node hours for a single workload running on a transient cluster aggregated at the date and workload type level. This data set may be used to help Databricks understand consumption patterns and user behaviors on our platform. For example, we can inspect this data to understand if a given customer prefers our `automated` or `interactive` features, or understand which AWS instance types are preferred among all of our customers. \n",
    "\n",
    "###Format: \n",
    " * JSON\n",
    " * Resides on S3\n",
    "\n",
    "###Schema:\n",
    "* date (String)\n",
    "* nodeHours (Double)\n",
    "* workloadType (String) (read more [here](https://databricks.com/product/aws-pricing#clusters))\n",
    "* metadata (Struct)\n",
    " * clusterMetadata (Struct): Describes the cluster configuration\n",
    " * runtimeMetadata (Struct): Describes the software configuration\n",
    " * workloadMetadata (Struct): Describes the customer. Each shard may have one or many workspaces and each workspace may have zero or many clusters \n",
    "\n",
    "### Part A: SparkSQL and Dataframes \n",
    "\n",
    "In this section, you'll read in data to create a dataframe in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this link for more details on how to use DBFS:\n",
    "https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfsExecute the command below to list the files in a directory that you will be analyzing.  There are several files in this test dataset.%fs ls /databricks-coding-challenge/workloads%fs head dbfs:/databricks-coding-challenge/workloads/part-00000-tid-7467717951814126607-30bac750-dd23-4160-a2a6-e57064ff0dc6-1506091-1-c000.json\n",
    "### Question 1 (15 points):\n",
    "Please create a temporary Spark SQL view called \"workloads\" from the json files in the directory listed up above%python\n",
    "df = (spark.read\n",
    "  .format(\"json\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"/databricks-coding-challenge/workloads/\")\n",
    ")\n",
    "df.createOrReplaceTempView(\"workloads\")\n",
    "What is the schema for this table?-- use describe command to get schema of this table\n",
    "-- alteratively can describe in python using df.printSchema()\n",
    "desc workloads-- Get an idea of overall size of the table \n",
    "-- SELECT COUNT(*) FROM workloads\n",
    "### Question 2 (15 points):\n",
    "\n",
    "Please print out all the unique workspaceId's for this dataset and order them such that workspaceId's are increasing in number.-- unique workspace id's in ascending order\n",
    "-- Q: workspaceId is string containing int values, do we need to order by their actual value? \n",
    "SELECT DISTINCT metadata.workloadMetadata.workspaceId\n",
    "FROM workloads\n",
    "ORDER BY abs(workspaceId)  --order by increasing in number-- sanity check \n",
    "-- Q: Why is no of unique workspaceId so less \n",
    "-- A: distribution is skewed, most are mapped to one workspaceId\n",
    "SELECT metadata.workloadMetadata.workspaceId,\n",
    "       count(*) as no_of_records\n",
    "FROM workloads\n",
    "GROUP BY metadata.workloadMetadata.workspaceId\n",
    "ORDER BY no_of_records DESC--sanity check 2 \n",
    "SELECT sum(no_of_records) as total_records\n",
    "FROM (\n",
    "  SELECT metadata.workloadMetadata.workspaceId,\n",
    "       count(*) as no_of_records\n",
    "  FROM workloads\n",
    "  GROUP BY metadata.workloadMetadata.workspaceId\n",
    "  ORDER BY no_of_records DESC\n",
    ")\n",
    "### Question 3 (15 points):\n",
    "\n",
    "What is the number of unique clusters in this data set?  A cluster is identified by the `metadata.workloadMetadata.clusterId` field.-- no of unique clusters in this data set \n",
    "SELECT COUNT(DISTINCT metadata.workloadMetadata.clusterId) AS unique_clusters \n",
    "FROM workloads### Question 4 (15 points): \n",
    "What is the number of workload hours each day for the workspaceID - `-9014487477555684744`?-- no of workload hours each day for workspaceID - -9014487477555684744 \n",
    "-- Assumption: using nodeHours as a proxy for workload hours \n",
    "SELECT date,\n",
    "       SUM(nodeHours) AS workload_hours\n",
    "FROM workloads \n",
    "WHERE metadata.workloadMetadata.workspaceId = '-9014487477555684744'\n",
    "GROUP BY date \n",
    "ORDER By date\n",
    "Determine how many nodes are spot vs. on demand for a given cluster.-- Get some cluster id which have both\n",
    "select metadata.workloadMetadata.clusterId,\n",
    "       collect_set(metadata.clusterMetadata.containerIsSpot) as value_set\n",
    "from workloads \n",
    "group by metadata.workloadMetadata.clusterId\n",
    "having size(value_set) = 2-- #  change the cluster id here to change the output in cmd24 \n",
    "CREATE WIDGET TEXT clusterId DEFAULT \"-1048771871094449110\"-- Determine how many nodes are spot vs. on demand for a given cluster.\n",
    "-- Sol: group by metadata.clusterMetadata.containerIsSpot for a given cluster id \n",
    "\n",
    "SELECT CASE WHEN metadata.clusterMetadata.containerIsSpot THEN 'spot'\n",
    "            ELSE 'on-demand'\n",
    "       END AS node_type,\n",
    "       count(*) as no_of_nodes\n",
    "FROM workloads WHERE metadata.workloadMetadata.clusterId = $clusterId\n",
    "GROUP BY metadata.clusterMetadata.containerIsSpotREMOVE WIDGET clusterId### Question 5 (15 points): \n",
    "\n",
    "How many interactive node hours per day are there on the different Spark versions over time.-- How many interactive node hours per day are there on the different Spark versions over time.\n",
    "SELECT metadata.runtimeMetadata.sparkVersion,\n",
    "       date,\n",
    "       SUM(nodeHours) AS total_node_hours\n",
    "FROM workloads \n",
    "WHERE LOWER(workloadType) = 'interactive'\n",
    "GROUP BY date,\n",
    "         metadata.runtimeMetadata.sparkVersion\n",
    "ORDER BY metadata.runtimeMetadata.sparkVersion,\n",
    "         date### Question 6 (25 points):\n",
    "#### TPC-H Dataset\n",
    "You're provided with a Line Items records from the TPC-H data set. The data is located in `/databricks-datasets/tpch/data-001/lineitem`.\n",
    "Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach.\n",
    "\n",
    "You're free to use any combinations of SparkSQL, PySpark or Scala Spark to answer this challenge.![](https://docs.deistercloud.com/mediaContent/Databases.30/TPCH%20Benchmark.90/media/tpch_schema.png?v=0)%python\n",
    "src ='/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl'\n",
    "schema =\", \".join(['orderkey int', 'partkey int', 'suppkey int', 'lineNumber int', 'quantity int', 'extendedprice float', 'discount float', 'tax float', 'returnflag string', 'linestatus string', 'shipdate date', 'commitdate date', 'receiptdate date', 'shipinstruct string', 'shipmode string', 'comment string'])\n",
    "tpc_h = (spark.read.format(\"csv\") \n",
    "          .schema(schema)\n",
    "          .option(\"header\", False)\n",
    "          .option(\"sep\", \"|\")\n",
    "          #.option(\"inferSchema\", True)\n",
    "          .load(src)\n",
    "        )\n",
    "%python\n",
    "dbutils.fs.head('/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl')%python\n",
    "display(tpc_h)%python\n",
    "tpc_h.createOrReplaceTempView('tpc_h')Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach.-- Q: How much info do we need to return for line item. Is order key enough? \n",
    "\n",
    "WITH line_item_ranked AS \n",
    "(\n",
    "SELECT orderkey,\n",
    "       partkey,\n",
    "       shipdate,\n",
    "       -- assuming if there are two shipped on the same date, then only those 2 are returned\n",
    "       row_number() OVER(PARTITION BY partkey ORDER BY shipDate DESC) AS most_recent_ranked\n",
    "FROM tpc_h\n",
    ")\n",
    "SELECT partkey,\n",
    "       orderkey,\n",
    "       shipdate \n",
    "FROM line_item_ranked \n",
    "WHERE most_recent_ranked <= 2 \n",
    "                       \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6d7c7b-03d7-462f-8373-ac685e4c4fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prediction = qa_module(context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33b8a1c-17cd-40b7-a531-a6e2dd828cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4883b84e-85d4-43a5-ba42-cd1e50c8e3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../helper/GradingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1fd060-05c3-4f9e-8f7c-fd960a8223b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SQLPrompt1(dspy.Signature):\n",
    "  \"\"\" Check if the provided context has any code snippet which creates a table/view called workloads and get shcema of workloads \"\"\"\n",
    "\n",
    "  text: str = dspy.InputField()\n",
    "\n",
    "  score: str = dspy.OutputField(desc=\"15 if any code snippet below creates a table/view and displays its schema, 7.5 if it creates a table/view but does not display its schema,  0 if no code snippet creates a table/view\")\n",
    "\n",
    "  code_snippet: str = dspy.OutputField(desc=\"provide the code snippet which creates a table/view and displays its schema encapsulated as a string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f9a665-d414-4265-8b1e-ed810a6765b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = dspy.ChainOfThought(SQLPrompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5f66ed-98a4-4565-9cce-e6a9c0d2b8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x.__dict__['predict'].__dict__['signature'].instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e122d1f3-d8e0-463e-88f4-2f2dd709ad49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "help(dspy.ChainOfThought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5f4ac4b-cb8f-45db-9ca7-d9409e19c09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Answer(dspy.Signature):\n",
    "  score: int = dspy.OutputField(desc=\"The score provided by the model\")\n",
    "  reasoning: str = dspy.OutputField(desc=\"The reason provided by the model for the score\")\n",
    "  code_snippet: str = dspy.OutputField(desc=\"\"\" supporting code snippet provided by the model for the score \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71161f4d-afc8-400d-9ad1-1f9427c4e367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a prediction\n",
    "actual_output = Answer(\n",
    "    score=\"15\",\n",
    "    reasoning=\"The code snippet provided creates a temporary Spark SQL view called \\\"workloads\\\" from the JSON files in the specified directory. The schema of this table can be obtained using the `DESCRIBE` command or the `printSchema()` method in Python\",\n",
    "    code_snippet=\"\"\" ```python\n",
    "                      df = (spark.read\n",
    "                        .format(\"json\")\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .load(\"/databricks-coding-challenge/workloads/\")\n",
    "                      )\n",
    "                      df.createOrReplaceTempView(\"workloads\")\n",
    "\n",
    "                      # Get the schema of the table\n",
    "                      desc workloads\n",
    "                      ``` \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe050ae9-f3e0-42b9-80a8-5144beecaeee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_pair = dspy.Example(question=SQLPrompt1, answer=actual_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9cb0c1-89a6-4488-af39-41c4d3b8d653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module = Module(databricks.sdk.WorkspaceClient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23851735-69b5-4159-ab11-cf1178e67208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module.set_module_dict({'module_1': dspy.ChainOfThought(SQLPrompt1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee040399-add3-44b2-ae01-f0f9f5bae0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "candidate_dict = [\n",
    "  (\"X\",\"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-sql/01-SQL-X\")\n",
    "]\n",
    "\n",
    "human_graded_dict = {\n",
    "  \"X\": [15]\n",
    "}\n",
    "# w = databricks.sdk.WorkspaceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32f5ce6b-7521-41a7-9414-e2eb4ebb1ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lm = dspy.LM('databricks/databricks-meta-llama-3-3-70b-instruct')\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2946a7-5608-436d-8605-a8ae2a64bbc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "context_path = candidate_dict[0][1]\n",
    "human_answers_list = human_graded_dict[candidate_dict[0][0]]\n",
    "table_name = \"users.abhay_jalisatgi.gen_ai_eval\"\n",
    "section = \"SQL\"\n",
    "candidate = \"X\"\n",
    "\n",
    "print(f\"Context path: {context_path}\\n, Human answers list: {human_answers_list}\\n, Table name: {table_name}\\n, Section: {section}\\n, Candidate: {candidate}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d960767a-63a4-4af1-a45a-931837616103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = module.get_error_and_answer_dict(context_path, human_answers_list, table_name, section, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d478faa-43cb-4ec9-a084-c3a294388d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results['answers_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9dc744-3e45-4b1f-92d3-289907ae3b0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "qa_pair.answer.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5940817c-b92e-4b7c-9661-16a09f34c022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_score(example, pred, trace=None):\n",
    "  actual_values = np.array(example.answer.score)\n",
    "  pred = np.array(pred)\n",
    "  return np.sqrt(np.mean((actual_values - pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0997d7-732f-4453-8ee5-95f3214428eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pred = results['answers_dict']['score'].tolist()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74cc65f0-0e23-4eef-8e54-8c14759fc150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(evaluate_score(qa_pair, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c27f61-7a91-4adb-b4b0-bda3faee41fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results['answers_dict'].display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe37687a-40d9-43e4-9f17-f7ee05b66049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from users.abhay_jalisatgi.gen_ai_eval limit 5 \").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a891aae1-c892-4726-82e7-0fdcc431b110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table users.abhay_jalisatgi.training_set as \n",
    "select distinct score\n",
    "      , code_snippet\n",
    "      , candidate\n",
    "      ,secton as section\n",
    "\n",
    "from users.abhay_jalisatgi.gen_ai_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58461e81-f6b7-4ae0-9cee-5d7e08a473c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"users.abhay_jalisatgi.training_set\")\n",
    "df = df.dropDuplicates([\"candidate\", \"section\"])\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"users.abhay_jalisatgi.training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e3bbf5-c1ad-4fb8-8e45-9f3866359776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.signatures import Signature\n",
    "from dspy.signatures.field import InputField, OutputField\n",
    "\n",
    "# Define a Signature class with a docstring\n",
    "class SentimentAnalysis(Signature):\n",
    "  \"\"\"Analyze sentiment in text\"\"\"\n",
    "  input_text = InputField(desc=\"Text to be analyzed\")\n",
    "  sentiment = OutputField(desc=\"Overall sentiment (positive, negative, neutral)\")\n",
    "  key_insights = OutputField(desc=\"Specific elements contributing to the sentiment\")\n",
    "\n",
    "# Function to extract docstring from a Signature class\n",
    "def get_signature_docstring(signature_class):\n",
    "    docstring = signature_class.__doc__\n",
    "    return docstring.strip() if docstring else \"No docstring available\"\n",
    "\n",
    "# Extract and print the docstring\n",
    "docstring = get_signature_docstring(SentimentAnalysis)\n",
    "print(\"Docstring of SentimentAnalysis:\")\n",
    "print(docstring)\n",
    "\n",
    "# Example of handling a Signature without a docstring\n",
    "class TopicAnalysis(Signature):\n",
    "    input_text = InputField(desc=\"Text to be analyzed\")\n",
    "    topics = OutputField(desc=\"Main topics identified\")\n",
    "\n",
    "docstring = get_signature_docstring(TopicAnalysis)\n",
    "print(\"\\nDocstring of TopicAnalysis:\")\n",
    "print(docstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c40fdd2-0166-4488-b66b-fd9dd702dfd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887243155611,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "test-notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
