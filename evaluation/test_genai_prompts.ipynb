{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a82f190-dbec-428c-b8a8-991f921373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nbformat --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e22fd1b3-205a-4751-be8f-977f86f02286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U dspy --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5925617-f35e-4aa2-a5dc-8bfba48a73ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555bb1a0-41dd-4982-82f8-08828d0c0dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../prompts/sql_prompts_with_module_DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58a0cc5-8ab5-4417-80b2-c35241f6223b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../helper/GradingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1ab6c4-5bcf-4ec7-a62e-e0d194bc5ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    # Define global variables to be used throughout the notebook \n",
    "\"\"\"\n",
    "\n",
    "global_candidate_dict = {\n",
    "  \"X\": {\n",
    "    f\"SQL\": \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-sql/test-cases/01-SQL-X-no-answer-q1\"\n",
    "  }\n",
    "}\n",
    "\n",
    "global_human_graded_dict = {\n",
    "  \"X\": {\n",
    "    f\"SQL\": [0,15,15,12,15,25]\n",
    "  }\n",
    "}\n",
    "\n",
    "global_llm_models_dict = {\n",
    "  # 'llm-405B': 'databricks/databricks-meta-llama-3-1-405b-instruct',\n",
    "  'llm-70B': 'databricks/databricks-meta-llama-3-3-70b-instruct'\n",
    "  # 'claude': \"databricks/databricks-claude-3-7-sonnet\"\n",
    "}\n",
    "\n",
    "global_module = Module(databricks.sdk.WorkspaceClient())\n",
    "global_module_to_check = \"SQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff69308-c32b-4f1d-ad55-f9aa025ddf17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(global_candidate_dict)\n",
    "print(global_human_graded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873dd63e-bb0c-4ca3-b14d-bb7b52bb2997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_scores(predictions, candidate, notebook_name):\n",
    "  print(\"evaluate scores\")\n",
    "  mismatch_dict = {}\n",
    "\n",
    "  human_grading = global_human_graded_dict[candidate][notebook_name]\n",
    "\n",
    "  scores_list = predictions['score'].tolist()\n",
    "  code_snippet_list = predictions['code_snippet'].tolist()\n",
    "  reasoning_list = predictions['chain_of_thought_reasoning'].tolist()\n",
    "\n",
    "  for i in range(0, len(human_grading)):\n",
    "    if scores_list[i]!= human_grading[i]:\n",
    "      mismatch_dict[f\"{candidate}_{notebook_name}_q{i+1}\"] = {\n",
    "        'predicted_score': scores_list[i],\n",
    "        'human_score': human_grading[i],\n",
    "        'code_snippet': code_snippet_list[i],\n",
    "        'reasoning': reasoning_list[i]\n",
    "      }\n",
    "  if not mismatch_dict:\n",
    "    print(\"No mismatch found\")\n",
    "    return None\n",
    "\n",
    "  print(mismatch_dict)\n",
    "  return mismatch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd52f75-3983-4f05-a000-7dfae44d3e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_section_with_llm(lm, context_path,  candidate, notebook_name):\n",
    "  \n",
    "  start_time = time.time()\n",
    "\n",
    "  # Set the module dictionary to be used. \n",
    "  # A module dictionary will have prompts for each question in a module \n",
    "  global_module.set_module_dict(global_module_to_check)\n",
    "\n",
    "  # Get results for the section\n",
    "  results = global_module.get_error_and_answer_dict(context_path)\n",
    "  \n",
    "\n",
    "  results['answers_dict']['candidate'] = candidate\n",
    "  results['answers_dict']['notebook_name'] = notebook_name\n",
    "  results['answers_dict']['context'] = results['context'] \n",
    "\n",
    "  # End time calculation \n",
    "  end_time = time.time()\n",
    "  execution_time = end_time - start_time\n",
    "  print(f\" time taken by model {llm_name} for execution of notebook {notebook_name} is {execution_time} seconds\")\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d662c4-de63-4f52-b243-a026a975a90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_candidate_with_llm(llm_name, candidate):\n",
    "\n",
    "  section_df = pd.DataFrame()\n",
    "  mismatch_list = [] \n",
    "\n",
    "  # Iterate through multiple notebooks of a candidate\n",
    "  for notebook_name, notebook_path in notebook_dict.items():\n",
    "    print(f\" Evaluating {notebook_name} for {candidate}\")\n",
    "    \n",
    "    # Evaluate result of a candidate's notebook\n",
    "    notebook_result = evaluate_section_with_llm(\n",
    "                                                  llm_name\n",
    "                                              , notebook_path\n",
    "                                              , candidate\n",
    "                                              , notebook_name\n",
    "                                              )\n",
    "    \n",
    "    # evaluate results for all notebooks of a candidate \n",
    "    evaluation_result = evaluate_scores(notebook_result['answers_dict'],  candidate, notebook_name)\n",
    "    \n",
    "    #concatenate results of all notebooks for a candidate \n",
    "    section_df = pd.concat([section_df, notebook_result['answers_dict']], ignore_index=True)\n",
    "\n",
    "    #combine results of all mismatch of a candidate\n",
    "    mismatch_list.append(evaluation_result)\n",
    "\n",
    "  return {\n",
    "    'result_df': section_df,\n",
    "    'mismatch_list': mismatch_list\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d960767a-63a4-4af1-a45a-931837616103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "num_iterations = 6  #Since num_iterations are used to create parallel threads and it is an I/O bound task. max(num_iterations) = 3 x VCPU in the instance\n",
    " \n",
    "candidate_dict = {} # To store end results for each model\n",
    "\n",
    "# For each candidate \n",
    "for candidate, notebook_dict in global_candidate_dict.items():\n",
    "  # Take the candidate name and corresponding notebook \n",
    "  llm_dict = {}\n",
    "  # iterate through multiple llms \n",
    "  for llm_name,llm_signature in global_llm_models_dict.items():\n",
    "\n",
    "    # dspy.settings can only be changed by the thread that initially configured it.\n",
    "    lm = dspy.LM(llm_signature, cache=False, temperature=0)\n",
    "    dspy.configure(lm=lm)\n",
    "\n",
    "    iteration_dict = {}\n",
    "    # For each llm run multiple iterations to check for consistency\n",
    "    # Use a thread pool to manage threads\n",
    "    with ThreadPoolExecutor(max_workers=num_iterations) as executor:\n",
    "        # run num_iterations threads in parallel\n",
    "        futures = {executor.submit(evaluate_candidate_with_llm, llm_name, candidate): i for i in range(1, num_iterations+1)}\n",
    "\n",
    "        # Collect results as threads complete\n",
    "        for future in futures:\n",
    "            count = futures[future]  # Get the loop count associated with this future\n",
    "            try:\n",
    "                result = future.result()  # Get the result returned by the thread\n",
    "                iteration_dict[f'iteration_{count}'] = result  # Store in dictionary\n",
    "            except Exception as e:\n",
    "                print(f\"Error in thread for count {count}: {e}\")\n",
    "\n",
    "    llm_dict[llm_name] = iteration_dict\n",
    "\n",
    "  candidate_dict[candidate] = llm_dict    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc6e7c7-7e1d-4f00-8938-867525975d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "candidate_name = 'X'\n",
    "llm_name = 'llm-70B'\n",
    "llm_results = candidate_dict[candidate_name][llm_name]\n",
    "# llm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "205a692e-bf35-4df4-a634-87460bb9bbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_iteration_combined_df = pd.DataFrame()\n",
    "for k, v in llm_results.items():\n",
    "  all_iteration_combined_df = pd.concat([all_iteration_combined_df, v['result_df']], ignore_index=True)\n",
    "all_iteration_combined_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9aaee84-ace6-42da-b519-81048df0cae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_mismatch_list = {k:v['mismatch_list'] for k,v in llm_results.items()}\n",
    "combined_mismatch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee0026a-5642-486b-a8e0-e85cae9af8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f\"users.abhay_jalisatgi.test_{question_with_no_answer}_{module_to_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7063f7-dd72-45fe-99b3-074eb5fd3983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(f\"users.abhay_jalisatgi.test_{question_with_no_answer}_{module_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdca6e43-4611-4a19-8709-535a232d754a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_genai_prompts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
