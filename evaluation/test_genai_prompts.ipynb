{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a82f190-dbec-428c-b8a8-991f921373dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install nbformat databricks-sdk[openai]==0.38.0 dspy --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555bb1a0-41dd-4982-82f8-08828d0c0dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../prompts/sql_prompts_with_DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58a0cc5-8ab5-4417-80b2-c35241f6223b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../helper/GradingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2586d01-753c-40d3-bbd0-feb1e0f24f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "module = Module(databricks.sdk.WorkspaceClient())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1bceb5-7a10-48c9-ba8b-6f9968089c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question_with_no_answer = \"Q6_no_answer\"\n",
    "notebook_url = \"/Workspace/Users/vibhor.nigam@databricks.com/interview-scripts/interview-grading/example-notebooks-sql/01-SQL-X-no-answer-q6\"\n",
    "human_grades = [15,15,15,12,15,0]\n",
    "module_to_check = \"SQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee040399-add3-44b2-ae01-f0f9f5bae0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "candidate_dict = {\n",
    "  \"X\": {\n",
    "    f\"SQL_{question_with_no_answer}\": notebook_url\n",
    "  }\n",
    "}\n",
    "\n",
    "human_graded_dict = {\n",
    "  \"X\": {\n",
    "    f\"SQL_{question_with_no_answer}\": human_grades\n",
    "  }\n",
    "}\n",
    "\n",
    "llm_models = {\n",
    "  # 'llm-405B': 'databricks/databricks-meta-llama-3-1-405b-instruct',\n",
    "  'llm-70B': 'databricks/databricks-meta-llama-3-3-70b-instruct'\n",
    "  # 'claude': \"databricks/databricks-claude-3-7-sonnet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff69308-c32b-4f1d-ad55-f9aa025ddf17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(candidate_dict)\n",
    "print(human_graded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873dd63e-bb0c-4ca3-b14d-bb7b52bb2997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_scores(predictions, human_grading, candidate, section):\n",
    "  mismatch_dict = {}\n",
    "\n",
    "  scores_list = predictions['score'].tolist()\n",
    "  code_snippet_list = predictions['code_snippet'].tolist()\n",
    "  reasoning_list = predictions['chain_of_thought_reasoning'].tolist()\n",
    "\n",
    "  for i in range(0, len(human_grading)):\n",
    "    if scores_list[i]!= human_grading[i]:\n",
    "      mismatch_dict[f\"{candidate}_{section}_q{i+1}\"] = {\n",
    "        'predicted_score': scores_list[i],\n",
    "        'human_score': human_grading[i],\n",
    "        'code_snippet': code_snippet_list[i],\n",
    "        'reasoning': reasoning_list[i]\n",
    "      }\n",
    "  if not mismatch_dict:\n",
    "    return None\n",
    "  \n",
    "  print(mismatch_dict)\n",
    "  return mismatch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d960767a-63a4-4af1-a45a-931837616103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "test_results = {}\n",
    "model_dict = {} # To store end results for each model\n",
    "\n",
    "# Do multiple runs to check consistency\n",
    "for i in range(0,20):  \n",
    "\n",
    "  # iterate through multiple llms \n",
    "  for k,v in llm_models.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through multiple candidates \n",
    "    for candidate, values in candidate_dict.items():\n",
    "      \n",
    "      # Store answers for a candidate\n",
    "      answers_dict = {}\n",
    "\n",
    "      # Store results which are not matching \n",
    "      candidate_mismatch = {}\n",
    "\n",
    "      # Iterate through multiple notebooks of a candidate\n",
    "      for section, context_path in values.items():\n",
    "\n",
    "        # Create a new instance of llm module to empty out the cache \n",
    "        lm = dspy.LM(llm_models[k], cache=False, temperature=0)\n",
    "        dspy.configure(lm=lm)\n",
    "        \n",
    "        # Set the module dictionary to be used. \n",
    "        # A module dictionary will have prompts for each question in a module \n",
    "        module.set_module_dict(module_to_check)\n",
    "        \n",
    "        # Get results for the section \n",
    "        results = module.get_error_and_answer_dict(context_path)\n",
    "        \n",
    "        # Add info for candidate, section and context in the answers \n",
    "        results['answers_dict']['candidate'] = candidate\n",
    "        results['answers_dict']['section'] = section\n",
    "        results['answers_dict']['context'] = results['context'] \n",
    "        \n",
    "        # Get results of a section and evaluate  \n",
    "        answers_dict[f'{candidate}_{section}'] = results['answers_dict']\n",
    "        answers_df = pd.concat(answers_dict.values(), ignore_index=True)\n",
    "        # answers_df.display()\n",
    "        candidate_mismatch[f'{candidate}_{section}'] = evaluate_scores(answers_df, human_graded_dict[candidate][section], candidate, section)\n",
    "\n",
    "        # End time calculation \n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\" time taken by model {k} for execution of section {section} is {execution_time} seconds\")\n",
    "        \n",
    "\n",
    "    model_dict[f'{k}_round_{i+1}'] = {'answers_dict': answers_dict, 'execution_time': execution_time, 'lm':lm, 'candidate_mismatch': candidate_mismatch}\n",
    "  print(f\" run completed {i+1} \\n\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b317f9-0e19-4ac5-8d77-cf27038d8112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mismatch_dict = {k:v['candidate_mismatch'] for k, v in model_dict.items()}\n",
    "mismatch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0954fb35-e708-4338-a49e-d1f3eacd8ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for k, v in mismatch_dict.items(): \n",
    "  temp_df = pd.concat(model_dict[k]['answers_dict'].values(), ignore_index=True)\n",
    "  df = pd.concat([df, temp_df], ignore_index=True)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee0026a-5642-486b-a8e0-e85cae9af8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "f\"users.abhay_jalisatgi.test_{question_with_no_answer}_{module_to_check}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7063f7-dd72-45fe-99b3-074eb5fd3983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df).write.mode(\"overwrite\").saveAsTable(f\"users.abhay_jalisatgi.test_{question_with_no_answer}_{module_to_check}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e27bc0c5-7f96-4feb-b9f0-1d01d368b6a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_genai_prompts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
