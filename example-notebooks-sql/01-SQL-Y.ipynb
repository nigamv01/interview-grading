{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4afaae5-c5ba-4079-bd52-b8552ce43d01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Databricks Coding Challenge - SQL\n",
    "### Note: All questions should be done using SQL language\n",
    "\n",
    "## Spark SQL and DataFrames \n",
    "\n",
    "In this section, you'll read in data to create a DataFrame in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this [link](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfs) for more details on how to use DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67089e7-c297-49cd-8a9c-3adae92e2ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Understanding the data set \n",
    "\n",
    "###Overview:\n",
    "The data set used throughout the coding assessment resembles telemetry data that any software as a service (SaaS) company might collect. One record represents the node hours for a single workload running on a transient cluster aggregated at the date and workload type level. This data set may be used to help Databricks understand consumption patterns and user behaviors on our platform. For example, we can inspect this data to understand if a given customer prefers our `automated` or `interactive` features, or understand which AWS instance types are preferred among all of our customers. \n",
    "\n",
    "###Format: \n",
    " * JSON\n",
    " * Resides on S3\n",
    "\n",
    "###Schema:\n",
    "* date (String)\n",
    "* nodeHours (Double)\n",
    "* workloadType (String) (read more [here](https://databricks.com/product/aws-pricing#clusters))\n",
    "* metadata (Struct)\n",
    " * clusterMetadata (Struct): Describes the cluster configuration\n",
    " * runtimeMetadata (Struct): Describes the software configuration\n",
    " * workloadMetadata (Struct): Describes the customer. Each shard may have one or many workspaces and each workspace may have zero or many clusters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e0718b2-c20f-43c0-b712-375128a5ac90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Part A: SparkSQL and Dataframes \n",
    "\n",
    "In this section, you'll read in data to create a dataframe in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this link for more details on how to use DBFS:\n",
    "https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50668a01-2735-43d7-85f4-5d3e0dbffdc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute the command below to list the files in a directory that you will be analyzing.  There are several files in this test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2616e64-c417-4f09-955a-08d15558a4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /databricks-coding-challenge/workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521d14f1-02c5-4bd3-ba5b-d08c175a0c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs head dbfs:/databricks-coding-challenge/workloads/part-00000-tid-7467717951814126607-30bac750-dd23-4160-a2a6-e57064ff0dc6-1506091-1-c000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7862fb18-2d24-4de5-9b76-f23c52d2fdc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 1 (15 points):\n",
    "Please create a temporary Spark SQL view called \"workloads\" from the json files in the directory listed up above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca3984a9-218c-4cbb-9b94-4de93a372ea7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TOD0\n",
    "-- Read external files directly using sql, temporary view is sesssion scoped we need re create the view if notebook session is restarted or detached. \n",
    "-- Both sql and spark supports reading files using glob pattern to search files. *.json will find all json files in the current directory. \n",
    "-- the external file listing are cached when it first runs use `REFRESH workloads`` to detect new files after the view was first ran\n",
    "create or replace temporary view workloads\n",
    "  as\n",
    "select\n",
    "  *\n",
    "from\n",
    "  JSON.`dbfs:/databricks-coding-challenge/workloads/*.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5aa4c3f-6a69-47a5-a11d-8f46cdab2c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "What is the schema for this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "308e20d7-0c15-4ad8-8478-c77d4c994dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "## schema of the view. normally we use %sql describe table_name but it does not support for temporary views\n",
    "spark.sql(\"select * from workloads\").dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05cd0d19-e46e-4519-ae19-d1a9ad1b8d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 2 (15 points):\n",
    "\n",
    "Please print out all the unique workspaceId's for this dataset and order them such that workspaceId's are increasing in number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29a610a6-dd73-488c-808b-643a6fca059b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "-- Use . to access nested fields in the sturct column  \n",
    "select distinct metadata.workloadMetadata.workspaceId as workspaceId\n",
    "from workloads\n",
    "order by workspaceId asc \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b4b135-312f-42d5-a24f-669d12b77d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 3 (15 points):\n",
    "\n",
    "What is the number of unique clusters in this data set?  A cluster is identified by the `metadata.workloadMetadata.clusterId` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebedf13c-f734-4ee3-951b-ce10e78f434f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "select\n",
    "  count(distinct metadata.workloadMetadata.clusterId) as unique_cluster_count\n",
    "from\n",
    "  workloads\n",
    "\n",
    "--- Total of 140,592 unique clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9119a98-1761-4955-94a3-50dff1bf43ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 4 (15 points): \n",
    "What is the number of workload hours each day for the workspaceID - `-9014487477555684744`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11534358-6080-4fa1-ae7c-038f06551a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (select\n  date,\n  sum(nodeHours) nodeHours\nfrom\n  workloads \nwhere workloadMetadata.workspaceId = -9014487477555684744\ngroup by date\norder by date asc) SELECT `date`,SUM(`nodeHours`) `column_1c482c426` FROM q GROUP BY `date`",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "date",
             "id": "column_1c482c424"
            },
            "y": [
             {
              "column": "nodeHours",
              "id": "column_1c482c426",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "percentValues": false,
            "stacking": null
           },
           "seriesOptions": {
            "column_1c482c4211": {
             "type": "column",
             "yAxis": 0
            },
            "column_1c482c426": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "c0e5f3da-69bd-488c-ab2d-f0c9c86b61cc",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "alias": "column_1c482c426",
           "args": [
            {
             "column": "nodeHours",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "select\n",
    "  date,\n",
    "  sum(nodeHours) nodeHours\n",
    "from\n",
    "  workloads \n",
    "where metadata.workloadMetadata.workspaceId = -9014487477555684744\n",
    "group by date\n",
    "order by date asc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "596bd2e0-0a8f-4de1-aee1-74cc01c5a527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Determine how many nodes are spot vs. on demand for a given cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf6cb059-2850-46c7-890c-1f2a7774cd01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--- Does same cluster id spins up multiple times across dates or multiple times a day? answwer is Yes, as shown below  query highlights number of times each cluster id spins up across days.\n",
    "\n",
    "select  metadata.workloadMetadata.clusterId, workloadtype, count(distinct date) NumUniqueDays\n",
    " from workloads\n",
    " group by metadata.workloadMetadata.clusterId, workloadtype\n",
    " having count(distinct date) > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b026d1d0-beba-433b-ac15-5f8efb7fa7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "-- Below query shows total number of spot vs On demand across all days\n",
    "-- for given cluster id -1001061315011396571, there were 48 nodes were spot and 6 nodes were not, this cluster was spun up once. \n",
    "-- for given cluster id -1210922869916467973, there were 43 nodes were spot and 53 nodes were not, this cluster was spun up across more than one day. see the next cell query that breaks down by day. \n",
    "select  metadata.workloadMetadata.clusterId, \n",
    "      sum(case when metadata.clusterMetadata.containerIsSpot =\"true\" then 1 else 0 end)  as NumberOfSpotNodes,\n",
    "      sum(case when metadata.clusterMetadata.containerIsSpot =\"false\" then 1 else 0 end)  as NumberOfOnDemandNodes,\n",
    "      count(*) as  NumberOfNodes, \n",
    "      count(distinct date) as TotalDistnctDates\n",
    "from workloads \n",
    "where metadata.workloadMetadata.clusterId in ( -1001061315011396571,-1210922869916467973)\n",
    "group by  \n",
    "      metadata.workloadMetadata.clusterId\n",
    "order by  metadata.workloadMetadata.clusterId\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c01a08-d51b-49ab-b55a-5f83a7f5f0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- for given cluster id -1210922869916467973, we can get a daily usage of spot vs on demand nodes since this cluster was spun up across many dates.\n",
    "select\n",
    "      date,   \n",
    "      metadata.workloadMetadata.clusterId, \n",
    "      sum(case when metadata.clusterMetadata.containerIsSpot =\"true\" then 1 else 0 end)  as NumberOfSpotNodes,\n",
    "      sum(case when metadata.clusterMetadata.containerIsSpot =\"false\" then 1 else 0 end)  as NumberOfOnDemandNodes,\n",
    "      count(*) as  NumberOfNodes\n",
    "\n",
    "from workloads \n",
    "where metadata.workloadMetadata.clusterId in (-1210922869916467973)\n",
    "group by  \n",
    "      date,  \n",
    "      metadata.workloadMetadata.clusterId\n",
    "order by  date, metadata.workloadMetadata.clusterId\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cf09ceb-bd0a-43aa-8cdb-acad3f237467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 5 (15 points): \n",
    "\n",
    "How many interactive node hours per day are there on the different Spark versions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32d29f4a-1538-4cc6-835e-298ebf65a2b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%sql WITH q AS (select\n  date,\n  runtimeMetadata.sparkVersion,\n  sum(nodehours) nodehours\nfrom\n  workloads\ngroup by\n  date,\n  runtimeMetadata.sparkVersion) SELECT `sparkVersion`,SUM(`nodehours`) `column_1c482c4221`,`date` FROM q GROUP BY `date`,`sparkVersion`",
       "commandTitle": "Daily Usage by Spark Version1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "date",
             "id": "column_1c482c4225"
            },
            "x": {
             "column": "sparkVersion",
             "id": "column_1c482c4220"
            },
            "y": [
             {
              "column": "nodehours",
              "id": "column_1c482c4221",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_1c482c4216": {
             "type": "column",
             "yAxis": 0
            },
            "column_1c482c4221": {
             "type": "column",
             "yAxis": 0
            },
            "nodehours": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "5bd3389f-ed24-45e6-91a9-e4f64eac5c24",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "sparkVersion",
           "type": "column"
          },
          {
           "column": "date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "sparkVersion",
           "type": "column"
          },
          {
           "alias": "column_1c482c4221",
           "args": [
            {
             "column": "nodehours",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "date",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "-- TODO\n",
    "--- 4.2.x-scala2.11 is most used compared to other spark versions in interactive workload. \n",
    "select\n",
    "  date,\n",
    "  metadata.runtimeMetadata.sparkVersion,\n",
    "  sum(nodehours) nodehours\n",
    "from\n",
    "  workloads\n",
    "where workloadType = 'interactive'\n",
    "group by\n",
    "  date,\n",
    "  metadata.runtimeMetadata.sparkVersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "260d0c23-256b-426c-9a10-98b7b293faa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 6 (25 points):\n",
    "#### TPC-H Dataset\n",
    "You're provided with a Line Items records from the TPC-H data set. The data is located in `/databricks-datasets/tpch/data-001/lineitem`.\n",
    "Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach.\n",
    "\n",
    "You're free to use any combinations of SparkSQL, PySpark or Scala Spark to answer this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7864d35-9acf-459a-8187-779a389aeb78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://docs.deistercloud.com/mediaContent/Databases.30/TPCH%20Benchmark.90/media/tpch_schema.png?v=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd3d290-8356-4e0c-a27f-c4acfaf4a4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "src ='/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl'\n",
    "schema =\", \".join(['orderkey int', 'partkey int', 'suppkey int', 'lineNumber int', 'quantity int', 'extendedprice float', 'discount float', 'tax float', 'returnflag string', 'linestatus string', 'shipdate date', 'commitdate date', 'receiptdate date', 'shipinstruct string', 'shipmode string', 'comment string'])\n",
    "tpc_h = (spark.read.format(\"csv\") \n",
    "          .schema(schema)\n",
    "          .option(\"header\", False)\n",
    "          .option(\"sep\", \"|\")\n",
    "          #.option(\"inferSchema\", True)\n",
    "          .load(src)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c614cf53-384f-424c-8698-eb9a87d2cdf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.head('/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2049105e-ec74-4c7b-b068-01b9a617d9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(tpc_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1a233ae-3a2a-4392-b9d6-45b31e38653a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "tpc_h.createOrReplaceTempView('tpc_h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f20887c-652a-4e16-a186-ae1603a2e251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "756521a4-7910-4edd-aeea-b23c6ad01ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The most effiecent way to do this is to use row_number() and partition by part_key and order by most recent shipdate and filter the rownumber upto 2 to get the top 2.\n",
    "- When we look at the spark execution plan most of the execution is driven by whostagecodegen except the exchange operator which is requred for sorting and cannot be avioded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22b12570-a8b0-4583-b526-b25f0f31f8c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "--- The most effiecent way to do this is to use row_number() when we partition by part_key and order by most recent shipdate and filter the rownumber upto 2 to get the top 2.\n",
    "--- When we look at the spark execution plan most of the execution is driven by whostagecodegen except the exchange operator which is requred for sorting and cannot be avioded.\n",
    "with cte as (\n",
    "  select\n",
    "    shipdate,\n",
    "    partkey,\n",
    "    row_number() over(\n",
    "      partition by partkey\n",
    "      order by\n",
    "        shipdate desc\n",
    "    ) as rank_most_shipdate_by_partkey\n",
    "  from\n",
    "    tpc_h\n",
    ")\n",
    "select\n",
    "  *\n",
    "from\n",
    "  cte\n",
    "where\n",
    "  rank_most_shipdate_by_partkey <= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aba3e8e-4f13-4fef-a269-ea80a18fdc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01-SQL-Y",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
