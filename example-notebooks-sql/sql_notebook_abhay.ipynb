{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf6324af-62f4-416d-8996-8363c7c958c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Databricks Coding Challenge - SQL\n",
    "### Note: All questions should be done using SQL language\n",
    "\n",
    "## Spark SQL and DataFrames \n",
    "\n",
    "In this section, you'll read in data to create a DataFrame in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this [link](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfs) for more details on how to use DBFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e7d326-f01e-47cc-a22e-a3a04eec8844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Understanding the data set \n",
    "\n",
    "###Overview:\n",
    "The data set used throughout the coding assessment resembles telemetry data that any software as a service (SaaS) company might collect. One record represents the node hours for a single workload running on a transient cluster aggregated at the date and workload type level. This data set may be used to help Databricks understand consumption patterns and user behaviors on our platform. For example, we can inspect this data to understand if a given customer prefers our `automated` or `interactive` features, or understand which AWS instance types are preferred among all of our customers. \n",
    "\n",
    "###Format: \n",
    " * JSON\n",
    " * Resides on S3\n",
    "\n",
    "###Schema:\n",
    "* date (String)\n",
    "* nodeHours (Double)\n",
    "* workloadType (String) (read more [here](https://databricks.com/product/aws-pricing#clusters))\n",
    "* metadata (Struct)\n",
    " * clusterMetadata (Struct): Describes the cluster configuration\n",
    " * runtimeMetadata (Struct): Describes the software configuration\n",
    " * workloadMetadata (Struct): Describes the customer. Each shard may have one or many workspaces and each workspace may have zero or many clusters \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06c423c2-6c01-4e6f-8fa0-cb8e944e447d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Part A: SparkSQL and Dataframes \n",
    "\n",
    "In this section, you'll read in data to create a dataframe in Spark.  We'll be reading in a dataset stored in the Databricks File System (DBFS).  Please see this link for more details on how to use DBFS:\n",
    "https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html#databricks-file-system-dbfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ca45082-c3d7-48fb-82ee-aeb982cc45c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Execute the command below to list the files in a directory that you will be analyzing.  There are several files in this test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa71894c-8644-48b9-84af-b13dc19cee67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /databricks-coding-challenge/workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf1463b4-845c-41fd-bf9d-531cdb4ba0bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs head dbfs:/databricks-coding-challenge/workloads/part-00000-tid-7467717951814126607-30bac750-dd23-4160-a2a6-e57064ff0dc6-1506091-1-c000.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9566e7f-45e3-4d45-836e-744c527ba5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 1 (15 points):\n",
    "Please create a temporary Spark SQL view called \"workloads\" from the json files in the directory listed up above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f67da38-91f5-4c18-99b3-f524d8cce06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "'''\n",
    "#Reading from json directory\n",
    "#creating a tempview\n",
    "#displaying the dataframe\n",
    "'''\n",
    "workloads_df = spark.read.json(\"dbfs:/databricks-coding-challenge/workloads\") \n",
    "workloads_df.createOrReplaceTempView('workloads')                         \n",
    "spark.sql(\"select * from workloads\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2a697c-e5c9-4386-a604-269d955ec2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "What is the schema for this table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fabe81d6-530c-4169-be73-e71251f89c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "workloads_df.printSchema() #printing the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "101907ca-0ac7-4257-ad61-54f666f9c791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 2 (15 points):\n",
    "\n",
    "Please print out all the unique workspaceId's for this dataset and order them such that workspaceId's are increasing in number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b5ef96-9593-4769-af67-37fdb9880e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Question 3 (15 points):\n",
    "\n",
    "What is the number of unique clusters in this data set?  A cluster is identified by the `metadata.workloadMetadata.clusterId` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac273f17-4a80-4eb4-9a8c-a88ee73dedd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "-- Count all the distinct cluster IDs\n",
    "select count(distinct metadata.workloadMetadata.clusterId) as no_of_unique_clusters from workloads;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dfdeab9-a360-4186-b310-93a49fecbdac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 4 (15 points): \n",
    "What is the number of workload hours each day for the workspaceID - `-9014487477555684744`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409460c4-41ca-4e49-9532-0167cbadb092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "-- List all the distinct workspace IDs by increasing order\n",
    "select date as day,round(sum(nodeHours),2) as workload_hours from workloads where metadata.workloadMetadata.workspaceId = '-9014487477555684744' group by date order by 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c3a9ce0-3917-44d2-9121-e5c67194c6d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Determine how many nodes are spot vs. on demand for a given cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f433af2-1636-4a2b-9103-5d3ab2416190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "select metadata.workloadMetadata.clusterId,sum(case when metadata.clusterMetadata.containerIsSpot = 'true' then 1 else 0 end) as spot,sum(case when metadata.clusterMetadata.containerIsSpot = 'false' then 1 else 0 end) as on_demand from workloads group by 1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86a596ec-b2a1-4cb8-96a8-18c1342f7d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 5 (15 points): \n",
    "\n",
    "How many interactive node hours per day are there on the different Spark versions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2210ec8-cfc7-43a0-942c-b6a0c787cd00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- TODO\n",
    "select date as day,metadata.runtimeMetadata.sparkVersion,round(sum(nodeHours),2) as interactive_node_hours from workloads where workloadType = 'interactive' group by 1,2 order by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b78384e-a322-4ca7-9e4c-bd0b3f64aa78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Question 6 (25 points):\n",
    "#### TPC-H Dataset\n",
    "You're provided with a Line Items records from the TPC-H data set. The data is located in `/databricks-datasets/tpch/data-001/lineitem`.\n",
    "Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach.\n",
    "\n",
    "You're free to use any combinations of SparkSQL, PySpark or Scala Spark to answer this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e22fd092-cbd9-4ab9-8617-e4be7292f10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://docs.deistercloud.com/mediaContent/Databases.30/TPCH%20Benchmark.90/media/tpch_schema.png?v=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf110b2-f24d-4505-ba21-9f6eeea3a3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "src ='/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl'\n",
    "schema =\", \".join(['orderkey int', 'partkey int', 'suppkey int', 'lineNumber int', 'quantity int', 'extendedprice float', 'discount float', 'tax float', 'returnflag string', 'linestatus string', 'shipdate date', 'commitdate date', 'receiptdate date', 'shipinstruct string', 'shipmode string', 'comment string'])\n",
    "tpc_h = (spark.read.format(\"csv\") \n",
    "          .schema(schema)\n",
    "          .option(\"header\", False)\n",
    "          .option(\"sep\", \"|\")\n",
    "          #.option(\"inferSchema\", True)\n",
    "          .load(src)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "513da415-bc1d-46ba-a99b-6d99823105a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.head('/databricks-datasets/tpch/data-001/lineitem/lineitem.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15c9d196-2071-4d3c-9a16-dadf6e6ec76d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(tpc_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ab3311f-20bb-45a9-9e33-8e9f44ff43d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "tpc_h.createOrReplaceTempView('tpc_h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fc6db0-88be-40e9-bde7-4a3cde85e3b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Find the top two most recently shipped (shipDate) Line Items per Part using the simplest and most efficient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967aff24-138d-4ed4-84cb-fd9465019544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "sql_notebook_abhay",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
